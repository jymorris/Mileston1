{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "2. Data Description:\n",
    "    1. Twitter Data\n",
    "        1. Columns we will be using\n",
    "        2. Limitations of the data\n",
    "    2. Covid Global data\n",
    "        1. Columns we will be using\n",
    "        2. Limitations of the data\n",
    "    3. Ethical Consideration\n",
    "3. Data Preperation, Cleaning and Manipulation:\n",
    "4. Exploratory Data Analysis (EDA):\n",
    "    1. Tweeter\n",
    "    2. COCID-19\n",
    "5. Sentiment Analysis:\n",
    "6. Correlation Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "As the pandemic has impacted the globe for 3 years and continued to reach repetitive peaks in different regions, epidemics and corresponding preventive measures have become the center of discussion and concerns to modern society. While previously it was much difficult to collect adequate reactions to vaccines across different communities at one place, twitter allows us to access expressed sentiments from a variety of communities.\n",
    "\n",
    "\n",
    "With that comes various opinions about the vaccine, and many choose to be vocal about their ideas on social media. We wondered whether looking at social media would help us understand what different communities across the world thought of the Vaccine, and whether such opinions are teetered by their socio-political climates and geographical locations.\n",
    "\n",
    "To do this, we turned to Twitter and scraped tweets containing hastags related to the vaccine. We then took these datapoints and quantified their approval ratings by calcuatinng sentiment score for each text. We then expressed this information in the form of graphs and maps. Currently, we have 5000 datapoints, and over 2000 datapoints with geographical coordinates.\n",
    "\n",
    "We hope that by reviewing these charts and maps, we can better understand the concerns different communitites have over the vaccine, and what may be contributing factors to this.\n",
    "\n",
    "The proposed project is to analyze tweets about the Pfizer-BioNTech vaccine in order to understand how sentiment varies by country, over time, and by demographic factors. The project also aims to study the correlation between sentiment and the number of confirmed cases, deaths, and active cases, and to compare the sentiment of tweets about the Pfizer-BioNTech vaccine to the sentiment of tweets about other COVID-19 vaccines. The goal is to gain insights on how people perceive and discuss the Pfizer-BioNTech vaccine on social media in different countries, how it changes over time and how it is influenced by various demographic factors. This information can be useful for researchers, healthcare professionals, and policymakers, to understand public opinion and to develop strategies to improve vaccine uptake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Twitter Data\n",
    "\n",
    "Our primary dataset is The Pfizer and BioNTech Vaccine Tweets Dataset posted on Kaggle and created by Gabriel Preda, who is a data scientist in Romania. \n",
    "\n",
    "Key features: Select 'user_location' 'text', 'hashtags' to extract useful information we want: locations for geo analysis, and their original tweets about the vaccines for text processing and sentiment analysis\n",
    "Estimated size: 4.54 MB\n",
    "Location: https://www.kaggle.com/gpreda/pfizer-vaccine-tweets\n",
    "Format: CSV file\n",
    "Access Method: through download or Kaggle API\n",
    "COLLECTION METHODOLOGY:\n",
    "Use tweepy to collect tweets about Pfizer & BioNTech (using #PfizerBioNTech hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 COVID-19 Global\n",
    "\n",
    "Our secondary dataset is the Covid-19 Global Dataset. The creator of the data set is an artificial intelligence engineer from Lebanon. \n",
    "\n",
    "Key features: useful columns to include are ‘total_confirmed’, ‘total_deaths’, ‘active_cases’, ‘, country’ to reflect the up-to-date numbers of daily confirmed, death and active cases for 218 countries\n",
    "Estimated size:20.38 kB (but with multiple versions)\n",
    "Location: https://www.kaggle.com/josephassaker/covid19-global-dataset?select=worldometer_coronavirus_summary_data.csv\n",
    "Format: CSV file\n",
    "Access Method: through download or Kaggle API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Ethical Concerns and bias\n",
    "\n",
    "User privacy protection,\n",
    "removed unique identifiers.\n",
    "\n",
    "Limitations of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preperation, Cleaning and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets:\n",
    "tweet_df = pd.read_csv(\"data/vaccination_tweets.csv\")\n",
    "covid_df = pd.read_csv(\"data/worldometer_coronavirus_daily_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for shape and missing values of the tweeter dataset\n",
    "print(tweet_df.shape)\n",
    "print(tweet_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for shape and missing values of the covid-19 stats dataset\n",
    "print(covid_df.shape)\n",
    "print(covid_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subseting the tweeter data\n",
    "tweet_df = tweet_df[[ 'user_name','user_location', 'user_created', 'date', 'text', 'hashtags', 'retweets', 'favorites']]\n",
    "# encode user_name using to integers according to ethical concerns 7202 unique usernames detected\n",
    "tweet_df['user_name'] = tweet_df['user_name'].factorize()[0]\n",
    "# change the format\n",
    "tweet_df['date'] = pd.to_datetime(tweet_df['date'], errors = 'coerce').dt.date\n",
    "tweet_df['user_created'] = pd.to_datetime(tweet_df['user_created'], errors = 'coerce').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df['date'] = pd.to_datetime(covid_df['date'], errors = 'coerce').dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Adding Counrty and City for joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we try to get the city if possible\n",
    "#!pip install geotext\n",
    "from geotext import GeoText\n",
    "# we will need the another tool to interact with Geotext\n",
    "from collections import OrderedDict\n",
    "def get_city(loc_txt):\n",
    "    try:\n",
    "        return GeoText(loc_txt).cities[0]\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def get_counrty(loc_txt):\n",
    "    try:\n",
    "        return list(GeoText(loc_txt).country_mentions.keys())[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "        \n",
    "tweet_df['city'] = tweet_df['user_location'].apply(get_city)\n",
    "tweet_df['country'] = tweet_df['user_location'].apply(get_counrty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second we get the country\n",
    "# !pip install pycountry\n",
    "import pycountry\n",
    "\n",
    "def fill_country(loc_txt, country):\n",
    "    if country!=None:\n",
    "        return country\n",
    "    candicates = []\n",
    "    candicates = [country.name for country in pycountry.countries if country.name in str(loc_txt)]\n",
    "    if candicates:\n",
    "        return candicates[0]\n",
    "    candicates = [country.alpha_2 for country in pycountry.countries if country.alpha_2 in str(loc_txt)]\n",
    "    if candicates:\n",
    "        return candicates[0]\n",
    "    candicates = [country.alpha_3 for country in pycountry.countries if country.alpha_3 in str(loc_txt)]\n",
    "    if candicates:\n",
    "        return candicates[0]\n",
    "    return None\n",
    "\n",
    "tweet_df['country'] = tweet_df[['user_location','country']].apply(lambda x:fill_country(x['user_location'],x['country']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# this may take about 5 mins\n",
    "# !pip install country_converter\n",
    "# convert country names to standard format\n",
    "import country_converter as coco\n",
    "tweet_df['country'] = tweet_df['country'].apply(lambda x: str(x))\n",
    "tweet_df['country'] = coco.convert(names=tweet_df['country'].to_list(), to='name_short')\n",
    "tweet_df['country'] = tweet_df['country'].apply(lambda x: None if x==\"not found\" else x)\n",
    "# there is nothing we can do about the error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Text Cleaning and sentiment evaluation\n",
    "Besides the spacial relationship, we are also interested in the content that users posted. To measure users' approval of the COVID-19 vaccine, we will approach with the sentiment score of the texts.\n",
    "\n",
    "The nltk library we will be using returns measures of postivity, negativity, neutrality, and a compound sentiment score of the text. The higher the compound sentiment score, the greater the approval.\n",
    "\n",
    "We will need to import the NLTK library and download some dictionaries to run certain methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# make all text lowercase\n",
    "tweet_df['clean_text'] = tweet_df.text.apply(lambda x: x.lower())\n",
    "\n",
    "#Remove twitter handlers\n",
    "tweet_df['clean_text'] = tweet_df['clean_text'].apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "\n",
    "#remove hashtags\n",
    "tweet_df['clean_text'] = tweet_df['clean_text'].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n",
    "\n",
    "# Remove URLS\n",
    "tweet_df['clean_text'] = tweet_df['clean_text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "\n",
    "# Remove all the special characters\n",
    "tweet_df['clean_text'] = tweet_df['clean_text'].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "\n",
    "#remove all single characters\n",
    "tweet_df['clean_text'] = tweet_df['clean_text'].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
    "\n",
    "# Substituting multiple spaces with single space\n",
    "tweet_df['clean_text'] = tweet_df['clean_text'].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "\n",
    "# removing short words\n",
    "tweet_df['clean_text'] = tweet_df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# downlaod some resouces\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# load the sentiment function\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "# load the stemmer function \n",
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# romve stop-wards\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# tokenization\n",
    "tokenized_tweet = tweet_df['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# remove stop-words\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# similar to stop words, we create a list of words we don't want\n",
    "unwanted_words = ['covid','vaccine']\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [w for w in x if w not in unwanted_words])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [w for w in x if not(w.find('http')!=-1)])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [w for w in x if not(w.find('vac')!=-1)])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [w for w in x if not(w.find('covid')!=-1)])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_tweet = []\n",
    "for i in range(len(tweet_df)):\n",
    "    t = ' '.join(tokenized_tweet[i])\n",
    "    detokenized_tweet.append(t)\n",
    "tweet_df['tweet_words'] = tokenized_tweet \n",
    "tweet_df['clean_text'] = detokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df[['text','tweet_words','clean_text']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will calculate the sentiment score for each tweet.\n",
    "\n",
    "tweet_df['compound_sentiment'] = tweet_df['clean_text'].apply(lambda x:sia.polarity_scores(x)['compound'])\n",
    "tweet_df['neg_sentiment'] = tweet_df['clean_text'].apply(lambda x:sia.polarity_scores(x)['neg'])\n",
    "tweet_df['pos_sentiment'] = tweet_df['clean_text'].apply(lambda x:sia.polarity_scores(x)['pos'])\n",
    "tweet_df['neu_sentiment'] = tweet_df['clean_text'].apply(lambda x:sia.polarity_scores(x)['neu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "plt.title('Distriubtion Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\n",
    "sns.kdeplot(tweet_df['neg_sentiment'],bw=0.1, label = 'neg_sentiment')\n",
    "sns.kdeplot(tweet_df['pos_sentiment'],bw=0.1, label = 'pos_sentiment')\n",
    "sns.kdeplot(tweet_df['neu_sentiment'],bw=0.1, label = 'neu_sentiment')\n",
    "sns.kdeplot(tweet_df['compound_sentiment'],bw=0.1, label = 'compound_sentiment')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = tweet_df.merge(covid_df, on=['country', 'date'], how='left')\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'], errors = 'coerce')\n",
    "merged_df['user_created'] = pd.to_datetime(merged_df['user_created'], errors = 'coerce')\n",
    "display(merged_df.sample(5))\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 User Demographics\n",
    "Created Time\n",
    "    Followers\n",
    "    Frequency by City and Country\n",
    "#### 4.2 Sentiment Score Over time\n",
    "#### 4.3 Sentiment Score VS Location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 User Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.displot(merged_df, x=\"user_created\", kde=True, color='blue',height=6, aspect=2,binwidth=30)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('# User Created', fontsize=12)\n",
    "plt.title('User Created Over Time', fontsize=15, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.displot(merged_df, x=\"user_created\", kde=True, color='blue',height=6, aspect=2, binwidth=30)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.xlim([datetime.date(2019, 1, 1), datetime.date(2022, 1, 1)])\n",
    "plt.ylabel('# User Created', fontsize=12)\n",
    "plt.title('User Created Over Time', fontsize=15, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the user demographics\n",
    "user_country = merged_df[['user_name', 'country']].value_counts().reset_index()\n",
    "top_10_countries =  user_country['country'].value_counts(sort=False).nlargest(10)\n",
    "\n",
    "sns.countplot(y=user_country['country'], order=top_10_countries.index, orient='h')\n",
    "plt.xlabel('# of Occurrences', fontsize=12)\n",
    "plt.ylabel('Country', fontsize=12)\n",
    "plt.title(\"User's Countries\", fontsize=15, fontweight='bold')\n",
    "# plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the user demographics\n",
    "top_10_countries =  merged_df['country'].value_counts(sort=False).nlargest(10)\n",
    "sns.countplot(y=merged_df['country'], order=top_10_countries.index, orient='h')\n",
    "plt.xlabel('Count', fontsize=12)\n",
    "plt.ylabel('Country', fontsize=12)\n",
    "plt.title('Distribution of Countries', fontsize=15, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_cities =  merged_df['city'].value_counts(sort=False).nlargest(10)\n",
    "sns.countplot(y=merged_df['city'], order=top_10_cities.index, orient='h')\n",
    "plt.xlabel('Count', fontsize=12)\n",
    "plt.ylabel('city', fontsize=12)\n",
    "plt.title('Distribution of city', fontsize=15, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_cities =  merged_df[merged_df.country=='United States']['city'].value_counts(sort=False).nlargest(10)\n",
    "sns.countplot(y=merged_df[merged_df.country=='United States']['city'], order=top_10_cities.index, orient='h')\n",
    "plt.xlabel('Count', fontsize=12)\n",
    "plt.ylabel('city', fontsize=12)\n",
    "plt.title('Distribution of U.S. city', fontsize=15, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the user demographics\n",
    "tweet_country = merged_df[['date', 'country']].value_counts().reset_index(name='tweets')\n",
    "top_5_countries =  user_country['country'].value_counts(sort=False).nlargest(5)\n",
    "tweet_country = tweet_country[tweet_country['country'].isin(top_5_countries.index)]\n",
    "\n",
    "# Make the joint plot\n",
    "plt.figure(figsize=(16, 20))\n",
    "\n",
    "sns.jointplot(x='date', y='tweets', data=tweet_country, \n",
    "              hue='country', height=15, \n",
    "              xlim = (datetime.date(2020, 11, 1), datetime.date(2022, 1, 1)), ylim=(0,40))\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Counts', fontsize=12)\n",
    "plt.title('Vaccine Tweets Discussion Over Time By Country', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two time point where things are interesting: 2021-2 and 2021-09\n",
    "# also why missing data and its impact on the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_country = merged_df[['date', 'country']].value_counts().reset_index(name='tweets')\n",
    "top_5_countries =  user_country['country'].value_counts(sort=False).nlargest(5)\n",
    "tweet_country = tweet_country[tweet_country['country'].isin(top_5_countries.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popular tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sentiment Score Over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import MonthEnd\n",
    "df = merged_df.copy()\n",
    "df['date'] = pd.to_datetime(df['date'], errors = 'coerce')\n",
    "# df[\"month_end_date\"] = df['date'].dt.date + MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the entire dataset in long-form mode will aggregate over repeated values (each year) to show the mean and 95% confidence interval:\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(x='date', y='compound_sentiment', data=df, color='blue')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('compound_sentiment', fontsize=12)\n",
    "plt.title('compound_sentiment Over Time', fontsize=15, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.groupby(['date']).mean().reset_index()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(x='date', y='neg_sentiment', data=df, color='blue')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('neg_sentiment', fontsize=12)\n",
    "plt.title('neg_sentiment Over Time', fontsize=15, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.groupby(['date']).mean().reset_index()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(x='date', y='pos_sentiment', data=df, color='blue')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('pos_sentiment', fontsize=12)\n",
    "plt.title('pos_sentiment Over Time', fontsize=15, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for compound snetiment score, the trend started to gain more volatility after 2021-07\n",
    "# we can also observe this in the negaive sentiment score chart, the peak in 2021-10 is due to less amount of data and thus we consider it outliers\n",
    "# the positive sentiment score seems more violent after 2021-9, probablit due to less data collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sentiment Score vs. Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()\n",
    "df = df.groupby([\"date\",\"country\"]).mean().reset_index()\n",
    "top_10_countries =  merged_df['country'].value_counts(sort=False).nlargest(5)\n",
    "df = df[(df['country'].isin(top_10_countries.index))&(df['pos_sentiment']>0)]\n",
    "\n",
    "# maybe group by week is better\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(x='date', y='pos_sentiment', data=df, hue='country')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('pos_sentiment', fontsize=12)\n",
    "plt.title('pos_sentiment per country over time', fontsize=15, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df = merged_df.copy()\n",
    "df = df[['date','country', 'compound_sentiment','neg_sentiment','pos_sentiment', 'neu_sentiment']].dropna()\n",
    "df = df.groupby(['country','date']).mean().reset_index()\n",
    "\n",
    "# df['date'] = pd.to_datetime(df['date'],format='%Y-%m-%d')\n",
    "\n",
    "start_date = df['date'].min()\n",
    "end_date = df['date'].max()\n",
    "\n",
    "\n",
    "fill_df = []\n",
    "for name, group in df.groupby('country'):\n",
    "    group.set_index('date', inplace=True)\n",
    "    reindexed = group.reindex(pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    reindexed['country'].fillna(name, inplace=True)\n",
    "    reindexed.fillna(0, inplace=True)\n",
    "    \n",
    "    fill_df.append(reindexed.reset_index())\n",
    "\n",
    "df = pd.concat(fill_df).rename(columns={'index':'date'})\n",
    "df['date'] = df['date'].astype(str)\n",
    "pos_df = df.copy()\n",
    "\n",
    "\n",
    "top_10_countries =  merged_df['country'].value_counts(sort=False).nlargest(10)\n",
    "df = df[(df['country'].isin(top_10_countries.index))].fillna(0)\n",
    "\n",
    "# Change in pos_sentiment over time for different countries\n",
    "fig = px.bar(df,\n",
    "             y = \"country\",\n",
    "             x = 'pos_sentiment',\n",
    "             animation_frame= 'date',\n",
    "             range_x = [0,1],\n",
    "             color='country'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly-express\n",
    "import plotly.express as px\n",
    "\n",
    "df = covid_df.copy()\n",
    "top_10_countries =  merged_df['country'].value_counts(sort=False).nlargest(10)\n",
    "df = df[(df['country'].isin(top_10_countries.index))].fillna(0)\n",
    "# maybe group by week is better\n",
    "\n",
    "\n",
    "fig = px.bar(df,\n",
    "             y = \"country\",\n",
    "             x = 'cumulative_total_cases',\n",
    "             animation_frame= 'date',\n",
    "             range_x = [0,50000000],\n",
    "             color='country'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 COVID-19 Global Cases vs. Pos Sentiment over the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly-express Viridis_r\n",
    "df = covid_df.copy()\n",
    "fig = px.choropleth(df,\n",
    "                    locations=\"country\",\n",
    "                    color=\"cumulative_total_cases\",\n",
    "                    hover_name=\"country\",\n",
    "                    animation_frame=\"date\",\n",
    "                    locationmode='country names',\n",
    "                    color_continuous_scale='Viridis_r',\n",
    "                    range_color=(1000, 2000000),\n",
    "                    height=600\n",
    "                    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in pos_sentiment over time for different countries\n",
    "df = pos_df.copy()\n",
    "fig = px.choropleth(df,\n",
    "                    locations=\"country\",\n",
    "                    color=\"pos_sentiment\",\n",
    "                    hover_name=\"country\",\n",
    "                    animation_frame=\"date\",\n",
    "                    locationmode='country names',\n",
    "                    color_continuous_scale='Viridis_r',\n",
    "                    range_color=(0,1),\n",
    "                    height=600\n",
    "                    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Cloud Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "df = merged_df.copy()\n",
    "text = ''\n",
    "for i in df['tweet_words']:\n",
    "    text += ' '.join(i)\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=800, background_color='white').generate(text)\n",
    "\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()\n",
    "df = df[['retweets','favorites', 'compound_sentiment','neg_sentiment',  'pos_sentiment',  'neu_sentiment',  'cumulative_total_cases',\n",
    "        'daily_new_cases',  'active_cases',  'cumulative_total_deaths','daily_new_deaths']].fillna(0)\n",
    "df_corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(16, 16),facecolor='w')\n",
    "\n",
    "sns.heatmap(df.corr(),annot=True, vmax=1, square=True, cmap=\"viridis\", fmt='.2g',annot_kws={\"fontsize\":12})\n",
    "plt.title('Correlation Heat Map')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['pos_sentiment', 'cumulative_total_cases', 'active_cases', 'daily_new_cases','daily_new_deaths']\n",
    "sns.pairplot(df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d62aae6e2c0355391e07203f9acfa55ab44d6d4cf6798945e603dd2b225c0541"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
